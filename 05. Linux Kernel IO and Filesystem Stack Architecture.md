## Linux Kernel I/O and Filesystem Stack Architecture

---
The Linux operating system employs a sophisticated, layered architecture for file I/O, spanning from the user application reading a document to writing bits to physical storage.

It is this modular design, known as the Linux Kernel I/O and Filesystem Stack, that enables Linux to natively support dozens of different file systems (Ext4, XFS, Btrfs) out of the box, while continuing to provide consistent behavior for applications. 

**Architecture Diagram**
```
┌─────────────────────────────────────────────────────────────────────────┐
│                     User Space Applications                             │
│           (glibc: fopen, read, write, mmap, io_uring)                  │
├═════════════════════════════════════════════════════════════════════════┤
│                     System Call Interface                               │
│           (sys_read, sys_write, sys_open, sys_io_uring)                │
├─────────────────────────────────────────────────────────────────────────┤
│                   Virtual Filesystem (VFS) Layer                        │
│     • Common file model (inode, dentry, file, superblock)              │
│     • Pathname resolution & namespace management                       │
│     • Permission checks (DAC/LSM)                                      │
├─────────────────────────────────────────────────────────────────────────┤
│     Dentry Cache        │       Inode Cache        │   [Buffer Cache]  │
│  (Path → inode mapping) │   (File metadata cache)  │  (Raw block cache)│
├─────────────────────────────────────────────────────────────────────────┤
│                    Page Cache (Memory Management Subsystem)             │
│    • File data caching in RAM pages                                    │
│    • Write-back/Write-through policies                                 │
│    • Memory mapped file support                                        │
├─────────────────────────────────────────────────────────────────────────┤
│                    Filesystem Implementation Layer                      │
│     Disk Based   │  Network Based │  Pseudo        │   Virtual         │
│    Ext4, XFS,    │  NFS, CIFS,    │  procfs,       │   tmpfs, ramfs,   │
│    Btrfs, FAT    │  CephFS        │  sysfs, debugfs│   FUSE            │
├─────────────────────────────────────────────────────────────────────────┤
│             Block I/O Transformation & Virtualization                   │
│     • Device Mapper (dm-linear, dm-crypt, dm-thin, dm-snapshot)        │
│     • MD (Multiple Device; Software RAID: RAID0,1,5,6,10)             │
│     • bcache, dm-cache (Caching layers)                                │
├─────────────────────────────────────────────────────────────────────────┤
│                    Generic Block Layer                                  │
│     • bio construction & lifecycle management                          │
│     • Request Queue (legacy) or blk-mq (multi-queue)                   │
│     • I/O Scheduling (none, mq-deadline, BFQ, Kyber)                   │
│     • Plugging for request merging & optimization                      │
├─────────────────────────────────────────────────────────────────────────┤
│              Block Device Driver Subsystem                             │
│     SCSI Stack   │   NVMe Driver   │   ATA/SATA     │   VirtIO-blk     │
│  (sd, sr, st)    │  (nvme, nvme-of)│  (ahci, libata)│  (virtio_blk)    │
├─────────────────────────────────────────────────────────────────────────┤
│                    Storage Hardware                                     │
│     HDD          │   SSD (SATA)    │   NVMe SSD     │   Persistent     │
│  (Mechanical)    │  (NAND Flash)   │  (PCIe/NVMe)   │   Memory         │
└─────────────────────────────────────────────────────────────────────────┘
```

**Explanation**

**1. User Space Applications**
- Run in unprivileged CPU mode (Ring 3/RISC-V U-mode/ARM EL0)
- Libraries are: glibc (POSIX API), libaio (async I/O), liburing (io_uring)
- Accessible via standard I/O (fread/fwrite), memory mapping (mmap), direct I/O (O_DIRECT), asynchronous I/O (io_uring)

**2. System Call Interface**
- Controlled transition between user/kernel spaces
- Its mechanism is SYSCALL/SYSENTER (x86), SVC (ARM), ECALL (RISC-V) instructions
- Functions are read, write, open, fsync, and io_uring_setup
- Responsible for parameter validation, pointer/user buffer handling, and security checks


**3. Virtual Filesystem (VFS) Layer**
- Abstraction layer providing a uniform file system interface
- **Data Structures**:
  - `struct super_block`: Filesystem instance metadata
  - `struct inode`: File metadata (permissions, size, data block pointers)
  - `struct dentry`: Directory entry cache (name → inode mapping)
  - `struct file`: Open file instance (position, flags)
- **Operations**:
  - `file_operations`: read, write, mmap, fsync
  - `inode_operations`: create, lookup, link, unlink
  - `dentry_operations`: name hashing, comparison

**4. Dentry & Inode Caches**
- **Dentry Cache**: Hash table for pathname → inode resolution
  - **Optimisation**: Negative dentry caching (non existent paths)
  - **LRU Management**: dcache shrinkers under memory pressure
- **Inode Cache**: Global hash table of active inodes
  - **Lifetime**: Reference counted, freed when last reference drops
- **Buffer Cache**: Caches raw disk blocks for metadata and direct block device access

**5. Page Cache**
- Central disk cache for file data
- **Implementation:** `address_space` objects attached to inodes
- **Features**:
  - **Read-ahead**: Prefetch sequential data
  - **Write-back**: Delayed writes with periodic flush (pdflush/kworker threads)
  - **Memory Pressure**: Managed by kswapd, uses LRU/active-inactive lists
- **Bypass Mechanisms**: 
  - `O_DIRECT`: Bypasses page cache (DMA directly to user buffers)
  - `O_SYNC`: Synchronous writes, cache used but immediately flushed

**6. Filesystem Implementation Layer**
- **Disk Based Filesystems**:
  - **Ext4**: Default for many distributions, journaling, extents
  - **XFS**: High performance, scalability, B+tree directories
  - **Btrfs**: Copy on write, snapshots, checksums, RAID like features
- **Network Filesystems**:
  - **NFS**: SunRPC based, stateless server design
  - **CIFS/SMB**: Microsoft protocol, stateful, locking
  - **CephFS**: Distributed, object based, CRUSH algorithm
- **Pseudo Filesystems**:
  - **procfs**: Process information (/proc/[pid]/)
  - **sysfs**: Kernel objects (/sys/)
  - **devtmpfs**: Device nodes
- **Virtual Filesystems**:
  - **tmpfs/ramfs**: RAM based, swap backed
  - **FUSE**: Userspace filesystem implementation

**7. Block I/O Transformation & Virtualization**
- **Device Mapper (dm)**:
  - `dm-linear`: Concatenation (LVM linear volumes)
  - `dm-crypt`: Transparent encryption (LUKS)
  - `dm-thin`: Thin provisioning with copy on write
  - `dm-snapshot`: Point in time snapshots
  - `dm-mirror`: Mirroring (RAID1 like)
- **MD (Multiple Device)**:
  - Software RAID: RAID0,1,5,6,10 implementations
  - Bitmap for resync tracking
- **Caching Layers**:
  - `bcache`: SSD caching for HDD
  - `dm-cache`: Device mapper caching layer

**8. Generic Block Layer**
- Basic I/O container (block range, direction, completion callback)
- **Queue Models**:
  - **Legacy Request Queue**: Single queue, elevator scheduling
  - **blk-mq (Multi-Queue)**: Per CPU/queue hardware queues, default for NVMe/SSD
- **I/O Schedulers**:
  - **none**: Direct dispatch (SSD optimised)
  - **mq-deadline**: Deadline guarantees, minimal starvation
  - **BFQ**: Fair queuing, desktop/server optimisation
  - **Kyber**: Token based, low latency
- **Request Plugging**: Temporary request batching for merging opportunities

**9. Block Device Driver Subsystem**
- **SCSI Subsystem**:
  - **Upper Layer**: sd (disk), sr (CD-ROM), st (tape)
  - **Mid Layer**: Error handling, retries, command queuing
  - **Lower Layer**: Host bus adapters (HBA drivers)
- **NVMe Driver**:
  - Admin and I/O Submission/Completion queues
  - Namespace management, multipath (NVMe over Fabrics)
- **ATA/SATA**:
  - `libata` library, `ahci` host controller driver
  - Native Command Queuing (NCQ)
- **VirtIO-blk**: Virtualization paravirtualized driver

**10. Storage Hardware**
- **HDD Characteristics**:
  - Seek time (~3-15ms), rotational latency (~2-8ms)
  - Optimized for sequential access, seek scheduling critical
- **SSD (SATA/NVMe)**:
  - NAND flash pages/blocks, wear leveling, garbage collection
  - TRIM/UNMAP for space reclamation
  - Over-provisioning for performance/endurance
- **NVMe Advantages**:
  - Parallelism (multiple queues, deep queue depths)
  - Lower latency (<100μs), PCIe bandwidth
  - Namespace sharing, multipath I/O
- **Persistent Memory (PMEM)**:
  - `DAX` (Direct Access) bypasses page cache
  - Memory mapped persistence

---
**A. Asynchronous I/O Pathways**:
- **io_uring**: Modern async I/O with ring buffers, reduces syscall overhead
- **KAIO**: Kernel AIO (limited filesystem support)
- **libaio**: Userspace async I/O library

**B. Memory Management Integration**:
- **Swap Cache**: Modified pages evicted to swap space
- **Memory Cgroups (memcg)**: Per cgroup page cache limits
- **Transparent Huge Pages (THP)**: 2MB pages for file backed memory

**C. Network Stack Integration**:
- **NFS/CIFS Client**: Convert filesystem ops to network packets
- **RDMA**: Remote Direct Memory Access for high speed networking

**D. Integrity & Encryption**:
- **dm-integrity**: Data integrity protection
- **fscrypt**: Filesystem level encryption
- **IMA/EVM**: Integrity Measurement Architecture

**E. Monitoring & Debugging**:
- **blktrace**: Block layer tracing
- **iostat, vmstat**: Performance monitoring
- **BPF/BCC**: Dynamic tracing and observability

---
**I/O Path Variations**

1. **Buffered Read Path**:
   ```
   read() → VFS → Page Cache (hit) → Copy to userspace
   read() → VFS → Page Cache (miss) → Filesystem → Block Layer → Driver → Hardware
   ```

2. **Direct I/O Path**:
   ```
   read(O_DIRECT) → VFS → Filesystem → Block Layer → DMA to user buffer
   ```

3. **Memory Mapped I/O**:
   ```
   mmap() → Page fault → VFS → Filesystem → Block Layer → Page mapped to process
   ```

4. **Writeback Path**:
   ```
   write() → Page Cache → Background flush (pdflush) → Filesystem → Block Layer
   ```
   ---
